{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasMatuszewski/Python-colab-notebooks/blob/main/notebooks/Bark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whuYJopH3MfR"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-TNlsYG5Z6c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install pip==24.0\n",
        "!pip install encodec\n",
        "!pip install funcy\n",
        "!pip install fairseq\n",
        "!pip install audiolm-pytorch==1.1.4\n",
        "!git clone https://github.com/serp-ai/bark-with-voice-clone.git\n",
        "%cd bark-with-voice-clone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTD3chqW3SkW"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knVYtTSkzDm9"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from encodec.utils import convert_audio\n",
        "from IPython.display import Audio\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# huBERT\n",
        "from hubert.hubert_manager import HuBERTManager\n",
        "from hubert.pre_kmeans_hubert import CustomHubert\n",
        "from hubert.customtokenizer import CustomTokenizer\n",
        "\n",
        "#Bark\n",
        "from bark.generation import (\n",
        "    load_codec_model,\n",
        "    generate_text_semantic\n",
        ")\n",
        "from bark.api import (\n",
        "    generate_audio,\n",
        "    generate_text_semantic\n",
        ")\n",
        "from bark.generation import (\n",
        "    SAMPLE_RATE,\n",
        "    preload_models,\n",
        "    codec_decode,\n",
        "    generate_coarse,\n",
        "    generate_fine,\n",
        "    generate_text_semantic\n",
        ")\n",
        "\n",
        "# Init\n",
        "nltk.download('punkt')\n",
        "use_gpu = torch.cuda.is_available()\n",
        "use_small = False\n",
        "preload_models(\n",
        "    text_use_gpu=use_gpu, text_use_small=use_small,\n",
        "    coarse_use_gpu=use_gpu, coarse_use_small=use_small,\n",
        "    fine_use_gpu=use_gpu, fine_use_small=use_small,\n",
        "    codec_use_gpu=use_gpu, force_reload=use_small,\n",
        "    path=\"models\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipes"
      ],
      "metadata": {
        "id": "X0xLRFM0BRB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clone(\n",
        "    path : str,\n",
        "    source_wav : str,\n",
        "    voice_name : str,\n",
        "    custom_hubert : str = 'data/models/hubert/hubert.pt',\n",
        "    custom_tokenizer : str = 'data/models/hubert/tokenizer.pth'\n",
        ") -> None:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    source_wav = path + '/voice_samples/' + source_wav + '.wav'\n",
        "    voice_name = path + '/encoded_voices/' + voice_name + '.npz'\n",
        "\n",
        "    hubert_manager = HuBERTManager()\n",
        "    hubert_manager.make_sure_hubert_installed()\n",
        "    hubert_manager.make_sure_tokenizer_installed()\n",
        "\n",
        "    hubert_model = CustomHubert(checkpoint_path=custom_hubert).to(device)\n",
        "    tokenizer = CustomTokenizer.load_from_checkpoint(custom_tokenizer).to(device)\n",
        "\n",
        "    model = load_codec_model(use_gpu=torch.cuda.is_available())\n",
        "    wav, source = torchaudio.load(source_wav)\n",
        "    wav = convert_audio(wav, source, model.sample_rate, model.channels).to(device)\n",
        "    semantic_vectors = hubert_model.forward(wav, input_sample_hz=model.sample_rate)\n",
        "    semantic_tokens = tokenizer.get_token(semantic_vectors)\n",
        "    with torch.no_grad():\n",
        "        encoded_frames = model.encode(wav.unsqueeze(0))\n",
        "    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze().cpu().numpy()\n",
        "    semantic_tokens = semantic_tokens.cpu().numpy()\n",
        "    np.savez(voice_name, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)"
      ],
      "metadata": {
        "id": "Q02lnaBM-0WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    path : str,\n",
        "    prompt : str,\n",
        "    voice_name : str,\n",
        "    temperature : float = 60,\n",
        "    top_k : int = 50,\n",
        "    top_p : float = 0.95,\n",
        ") -> np.ndarray:\n",
        "    path = path + '/encoded_voices/' + voice_name + '.npz'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    x_semantic = generate_text_semantic(\n",
        "        prompt,\n",
        "        history_prompt=path,\n",
        "        temp=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    x_coarse_gen = generate_coarse(\n",
        "        x_semantic,\n",
        "        history_prompt=path,\n",
        "        temp=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    x_fine_gen = generate_fine(\n",
        "        x_coarse_gen,\n",
        "        history_prompt=path,\n",
        "        temp=temperature,\n",
        "    )\n",
        "    return codec_decode(x_fine_gen)"
      ],
      "metadata": {
        "id": "kL6kwHhaALqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qlCbcCX3Ihy"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_clone(\n",
        "    path='/content/drive/MyDrive/DriveBlob/TTS',\n",
        "    source_wav='us_female_deepvoice_narrator',\n",
        "    voice_name='us_female_deepvoice_narrator',\n",
        ")"
      ],
      "metadata": {
        "id": "Jf8SsP_fBWfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEN_TEMP = 0.80\n",
        "TOP_P = 0.95\n",
        "TOP_K = 50"
      ],
      "metadata": {
        "id": "Q6tUovEqsHpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wav = generate(\n",
        "    path='/content/drive/MyDrive/DriveBlob/TTS',\n",
        "    prompt=\"Every dwarf who has ever interacted with humans before says the same thing: they look like poorly drawn caricatures, their faces contort in exaggerated expressions and mannerisms.\",\n",
        "    voice_name='us_female_deepvoice_narrator',\n",
        "    temperature=GEN_TEMP, top_k=TOP_K, top_p=TOP_P,\n",
        ")\n",
        "Audio(wav, rate=SAMPLE_RATE)"
      ],
      "metadata": {
        "id": "4868zxzMrmBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL0GnsyBWK6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}